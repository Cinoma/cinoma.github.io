import{_ as t,c as r,o,ag as a}from"./chunks/framework.UNo1JXUQ.js";const u=JSON.parse('{"title":"Featured Projects","description":"","frontmatter":{"layout":"doc","title":"Featured Projects"},"headers":[],"relativePath":"projects/index.md","filePath":"projects/index.md"}'),n={name:"projects/index.md"};function s(i,e,l,c,d,p){return o(),r("div",null,e[0]||(e[0]=[a('<h1 id="featured-projects" tabindex="-1">Featured Projects <a class="header-anchor" href="#featured-projects" aria-label="Permalink to &quot;Featured Projects&quot;">​</a></h1><h2 id="project-name-url-shortener-v1" tabindex="-1">Project Name: URL Shortener v1 <a class="header-anchor" href="#project-name-url-shortener-v1" aria-label="Permalink to &quot;Project Name: URL Shortener v1&quot;">​</a></h2><div class="info custom-block"><p class="custom-block-title">Project Details</p><ul><li><strong>Tech Stack:</strong> Vite, React, MaterialUI, Deno, MongoDB</li><li><strong>GitHub:</strong> <a href="https://github.com/Cinoma/url-shortener-v1" target="_blank" rel="noreferrer">Link to repository</a></li><li><strong>Live Demo:</strong> <a href="https://url-shortener-v1.deno.dev/" target="_blank" rel="noreferrer">View demo</a></li></ul></div><p>I built this project to showcase my prowess in full-stack development. It offers a responsive design for mobile and web, supports dark mode, operates locally and online, validates URLs, stores and retrieves data, and redirects users to long URLs.</p><h2 id="project-name-containerized-ai" tabindex="-1">Project Name: Containerized AI <a class="header-anchor" href="#project-name-containerized-ai" aria-label="Permalink to &quot;Project Name: Containerized AI&quot;">​</a></h2><div class="info custom-block"><p class="custom-block-title">Project Details</p><ul><li><strong>Tech Stack:</strong> Docker, Ollama, Open WebUI</li><li><strong>GitHub:</strong> <a href="https://github.com/Cinoma/ollama_open-webui" target="_blank" rel="noreferrer">Link to repository</a></li><li><strong>Live Demo:</strong> N/A</li></ul></div><p>I built this project to streamline AI model deployment and interaction, offering a Docker-based setup for Ollama with Open WebUI. It supports GPU acceleration, robust security measures like memory limits and read-only filesystems, and operates locally, making it ideal for securely managing and interacting with the latest large language models like DeepSeek and Qwen. Models can be accessed through Open WebUI&#39;s modern UI on port 3000 or directly via port 11434 for API interactions.</p>',7)]))}const h=t(n,[["render",s]]);export{u as __pageData,h as default};
